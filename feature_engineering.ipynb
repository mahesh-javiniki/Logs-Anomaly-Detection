{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31e350e7",
   "metadata": {},
   "source": [
    "# Feature Engineering - Anomaly Detection Dataset\n",
    "\n",
    "## Overview\n",
    "This notebook implements comprehensive feature engineering on the anomaly detection assignment dataset. The goal is to transform raw log entries into meaningful features that capture suspicious patterns, behavioral anomalies, and contextual information.\n",
    "\n",
    "## Feature Engineering Strategy\n",
    "\n",
    "1. **Duplicate Handling**: Identify and track duplicate records while creating metadata features\n",
    "2. **Temporal Features**: Extract time-based patterns (hour of day, weekday/weekend, business hours)\n",
    "3. **Missing Value Engineering**: Convert missing data into informative features\n",
    "4. **Aggregation Features**: Create user-level, app-level, and log-type behavioral statistics\n",
    "5. **Rarity Indicators**: Flag rare/unusual values across multiple dimensions\n",
    "\n",
    "## Why This Matters for Anomaly Detection\n",
    "\n",
    "Anomaly detection requires understanding **what's normal** to identify **what's abnormal**. Raw log data lacks context, feature engineering provides:\n",
    "\n",
    "- **Behavioral Context**: How does this activity compare to typical user/app behavior?\n",
    "- **Pattern Recognition**: Are there temporal patterns, frequency spikes, or unusual combinations?\n",
    "- **Rarity Signals**: Is this operation/browser/country/ISP rare enough to be suspicious?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfc3d930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b847b735",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Inspection\n",
    "\n",
    "**Purpose**: Load the dataset and perform initial data quality checks to understand the structure, size, and basic characteristics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2872e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_type</th>\n",
       "      <th>wf_principal_id</th>\n",
       "      <th>app_id</th>\n",
       "      <th>atype</th>\n",
       "      <th>browser</th>\n",
       "      <th>browser_version</th>\n",
       "      <th>device_os</th>\n",
       "      <th>device_os_version</th>\n",
       "      <th>uatype</th>\n",
       "      <th>asn</th>\n",
       "      <th>isp</th>\n",
       "      <th>country</th>\n",
       "      <th>frequency</th>\n",
       "      <th>log_timestamp</th>\n",
       "      <th>error_status</th>\n",
       "      <th>normalized_user_agent</th>\n",
       "      <th>operation</th>\n",
       "      <th>rw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ms_graph</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d60c33e1-c026-4f42-a243-213222885d01</td>\n",
       "      <td>app</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8075.0</td>\n",
       "      <td>Microsoft Limited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-12-07 20:59:01.178157</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>domains</td>\n",
       "      <td>read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ms_signin</td>\n",
       "      <td>user15</td>\n",
       "      <td>d60c33e1-c026-4f42-a243-213222885d01</td>\n",
       "      <td>app</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>20115.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-12-07 20:50:59.738144</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ms_graph</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d60c33e1-c026-4f42-a243-213222885d01</td>\n",
       "      <td>app</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8075.0</td>\n",
       "      <td>Microsoft Limited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-12-07 20:50:59.738144</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>domains</td>\n",
       "      <td>read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ms_graph</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d60c33e1-c026-4f42-a243-213222885d01</td>\n",
       "      <td>app</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8075.0</td>\n",
       "      <td>Microsoft Limited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-12-07 20:50:59.738144</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>organization</td>\n",
       "      <td>read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ms_graph</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d60c33e1-c026-4f42-a243-213222885d01</td>\n",
       "      <td>app</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8075.0</td>\n",
       "      <td>Microsoft Limited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-12-07 15:49:00.8726</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>domains</td>\n",
       "      <td>read</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    log_type wf_principal_id                                app_id atype  \\\n",
       "0   ms_graph             NaN  d60c33e1-c026-4f42-a243-213222885d01   app   \n",
       "1  ms_signin          user15  d60c33e1-c026-4f42-a243-213222885d01   app   \n",
       "2   ms_graph             NaN  d60c33e1-c026-4f42-a243-213222885d01   app   \n",
       "3   ms_graph             NaN  d60c33e1-c026-4f42-a243-213222885d01   app   \n",
       "4   ms_graph             NaN  d60c33e1-c026-4f42-a243-213222885d01   app   \n",
       "\n",
       "  browser browser_version device_os  device_os_version  uatype      asn  \\\n",
       "0     NaN             NaN       NaN                NaN       0   8075.0   \n",
       "1     NaN             NaN       NaN                NaN       0  20115.0   \n",
       "2     NaN             NaN       NaN                NaN       0   8075.0   \n",
       "3     NaN             NaN       NaN                NaN       0   8075.0   \n",
       "4     NaN             NaN       NaN                NaN       0   8075.0   \n",
       "\n",
       "                 isp country  frequency               log_timestamp  \\\n",
       "0  Microsoft Limited     NaN          1  2025-12-07 20:59:01.178157   \n",
       "1                NaN      US          1  2025-12-07 20:50:59.738144   \n",
       "2  Microsoft Limited     NaN          1  2025-12-07 20:50:59.738144   \n",
       "3  Microsoft Limited     NaN          1  2025-12-07 20:50:59.738144   \n",
       "4  Microsoft Limited     NaN          1    2025-12-07 15:49:00.8726   \n",
       "\n",
       "   error_status normalized_user_agent     operation    rw  \n",
       "0             1                   NaN       domains  read  \n",
       "1             0                   NaN           NaN   NaN  \n",
       "2             1                   NaN       domains  read  \n",
       "3             1                   NaN  organization  read  \n",
       "4             1                   NaN       domains  read  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/anomaly_detection_assignment.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ded328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preserve original row numbers from source CSV\n",
    "# This allows tracing any record back to the original file after all transformations\n",
    "df['original_index'] = df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd61f17",
   "metadata": {},
   "source": [
    "### Preserve Original Index for Traceability\n",
    "\n",
    "**Critical Design Decision**: Before any transformations, we create an explicit `original_index` column to preserve the original CSV row numbers.\n",
    "\n",
    "**Why an explicit column instead of DataFrame index?**\n",
    "\n",
    "1. **Survives all operations**: Column data is more resilient than index across merges, concatenations, and transformations\n",
    "2. **Explicit is better than implicit**: No need to remember `index_col=0` when loading CSVs\n",
    "3. **Cross-file consistency**: When sharing data across different scripts/notebooks, the original row reference is always present\n",
    "4. **User-friendly**: Anyone opening the CSV file immediately sees which source rows correspond to each record\n",
    "5. **Prevents accidental loss**: Users won't accidentally reset the index and lose traceability\n",
    "\n",
    "**Impact**: After duplicate removal (which eliminates ~75% of records), this column allows us to trace any detected anomaly back to its exact row in `anomaly_detection_assignment.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4a3fcd",
   "metadata": {},
   "source": [
    "### Initial Data Exploration\n",
    "\n",
    "Checking the dataset dimensions to understand the scale of data we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "472e40c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63713, 19)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aee3ce",
   "metadata": {},
   "source": [
    "### Data Types and Structure\n",
    "\n",
    "Understanding the column types, non-null counts, and memory usage helps us plan our feature engineering approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3371f588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 63713 entries, 0 to 63712\n",
      "Data columns (total 19 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   log_type               63713 non-null  object \n",
      " 1   wf_principal_id        63470 non-null  object \n",
      " 2   app_id                 63713 non-null  object \n",
      " 3   atype                  63713 non-null  object \n",
      " 4   browser                49394 non-null  object \n",
      " 5   browser_version        23375 non-null  object \n",
      " 6   device_os              19593 non-null  object \n",
      " 7   device_os_version      4536 non-null   float64\n",
      " 8   uatype                 63713 non-null  int64  \n",
      " 9   asn                    63690 non-null  float64\n",
      " 10  isp                    63555 non-null  object \n",
      " 11  country                17377 non-null  object \n",
      " 12  frequency              63713 non-null  int64  \n",
      " 13  log_timestamp          63713 non-null  object \n",
      " 14  error_status           63713 non-null  int64  \n",
      " 15  normalized_user_agent  48854 non-null  object \n",
      " 16  operation              46337 non-null  object \n",
      " 17  rw                     45702 non-null  object \n",
      " 18  original_index         63713 non-null  int64  \n",
      "dtypes: float64(2), int64(4), object(13)\n",
      "memory usage: 9.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d144de",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Duplicate Detection and Handling\n",
    "\n",
    "**Purpose**: Identify and analyze duplicate records to improve data quality and prevent bias in anomaly detection models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff3a1b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact duplicates found: 48,116 (75.52%)\n",
      "Unique rows: 15,597 (24.48%)\n"
     ]
    }
   ],
   "source": [
    "# Get all columns except original_index for duplicate detection\n",
    "cols_for_duplicate_check = [col for col in df.columns if col != 'original_index']\n",
    "\n",
    "duplicate_count = df.duplicated(subset=cols_for_duplicate_check).sum()\n",
    "print(f\"Exact duplicates found: {duplicate_count:,} ({duplicate_count/len(df)*100:.2f}%)\")\n",
    "print(f\"Unique rows: {len(df.drop_duplicates(subset=cols_for_duplicate_check)):,} ({len(df.drop_duplicates(subset=cols_for_duplicate_check))/len(df)*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77824db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track which rows were duplicated (before removal) as a potential feature\n",
    "# Exclude original_index from duplicate detection\n",
    "cols_for_duplicate_check = [col for col in df.columns if col != 'original_index']\n",
    "df['was_duplicated'] = df.duplicated(subset=cols_for_duplicate_check, keep=False).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f728ca",
   "metadata": {},
   "source": [
    "### Create Duplicate Indicator Feature\n",
    "\n",
    "Using `keep=False` marks ALL duplicates (not just the second occurrence), capturing the full extent of duplication.This binary feature will be valuable for the ML model to identify patterns related to duplicated logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e906b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates while preserving original_index of the first occurrence\n",
    "cols_for_duplicate_check = [col for col in df.columns if col not in ['original_index', 'was_duplicated']]\n",
    "df = df.drop_duplicates(subset=cols_for_duplicate_check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f05d365e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing duplicates, new shape: (15597, 20)\n"
     ]
    }
   ],
   "source": [
    "# after removing duplicates\n",
    "print(f\"After removing duplicates, new shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6777915",
   "metadata": {},
   "source": [
    "### Duplicate Handling Details\n",
    "\n",
    "**Critical Findings:**\n",
    "- **Exact Duplicates**: 48,116 records (75.52% of original data)\n",
    "  - Only 15,597 unique rows remain (24.48%)\n",
    "  - Extremely high duplication rate indicates potential data quality issues or logging artifacts\n",
    "\n",
    "- **Duplicates Excluding Timestamp**: 58,187 records (91.33%)\n",
    "  - Even when ignoring timestamp differences, 91% of data is duplicated\n",
    "  - Suggests systematic data collection redundancy or repeated extracts\n",
    "\n",
    "**Data Reduction Impact:**\n",
    "- **Original Size**: 63,713 rows\n",
    "- **After Deduplication**: 15,597 rows (75.52% reduction)\n",
    "- **Records Removed**: 48,116 rows\n",
    "- **New Dimensions**: 15,597 rows × 19 columns (added `was_duplicated` feature)\n",
    "\n",
    "**Decision Rationale for Removing Duplicates:**\n",
    "1. **Problem Context**: \"Each row represents a set of logs\" - data is already aggregated\n",
    "2. **Frequency Field**: Already captures log counts, making row duplication redundant\n",
    "3. **Model Bias Prevention**: Duplicates would overweight normal patterns in anomaly detection algorithms\n",
    "4. **Data Integrity**: Identical timestamps + all fields suggest data quality issue rather than legitimate repeated events\n",
    "5. **Target**: Detecting anomalous ROWS, not individual logs\n",
    "\n",
    "**Feature Engineering:**\n",
    "- Created `was_duplicated` binary indicator (1 if row had duplicates, 0 otherwise)\n",
    "- Preserves information about duplication patterns for potential anomaly signals\n",
    "- May help identify systematic data collection issues or repeated user behaviors\n",
    "\n",
    "**Implications for Analysis:**\n",
    "- The 75.52% duplication rate is unusually high for production log data\n",
    "- Suggests the original dataset may contain repeated extracts or logging artifacts\n",
    "- Cleaner dataset (15,597 rows) is more appropriate for pattern discovery and anomaly detection\n",
    "- Must remain vigilant about potential information loss, though frequency field mitigates this risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598d5905",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Temporal Feature Engineering\n",
    "\n",
    "**Purpose**: Extract time-based features from timestamps to enable temporal pattern analysis and detect time-related anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f041a183",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_timestamp'] = pd.to_datetime(df['log_timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935b518a",
   "metadata": {},
   "source": [
    "Convert timestamp column to datetime format for temporal feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49f2a44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour'] = df['log_timestamp'].dt.hour\n",
    "df['day_of_week'] = df['log_timestamp'].dt.dayofweek\n",
    "df['day_of_month'] = df['log_timestamp'].dt.day\n",
    "df['is_weekend'] = (df['day_of_week'].isin([5, 6])).astype(int)\n",
    "df['is_business_hours'] = ((df['hour'] >= 9) & (df['hour'] < 18)).astype(int)\n",
    "df['is_night_hours'] = (df['hour'] < 6).astype(int)\n",
    "df['is_late_night'] = ((df['hour'] >= 0) & (df['hour'] < 3)).astype(int)\n",
    "\n",
    "# Hour categories\n",
    "def categorize_hour(hour):\n",
    "    if hour < 6:\n",
    "        return 'night'\n",
    "    elif hour < 12:\n",
    "        return 'morning'\n",
    "    elif hour < 18:\n",
    "        return 'afternoon'\n",
    "    else:\n",
    "        return 'evening'\n",
    "\n",
    "df['hour_category'] = df['hour'].apply(categorize_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688fd63e",
   "metadata": {},
   "source": [
    "### Create Comprehensive Temporal Features\n",
    "\n",
    "**Feature Breakdown:**\n",
    "\n",
    "| Feature | Type | Purpose |\n",
    "|---------|------|---------|\n",
    "| `hour` | Numerical (0-23) | Capture hourly patterns |\n",
    "| `day_of_week` | Numerical (0-6) | Monday=0, Sunday=6 |\n",
    "| `day_of_month` | Numerical (1-31) | Monthly patterns |\n",
    "| `is_weekend` | Binary | Flag Saturday/Sunday activity |\n",
    "| `is_business_hours` | Binary | Flag 9 AM - 6 PM activity |\n",
    "| `is_night_hours` | Binary | Flag 12 AM - 6 AM activity (high suspicion) |\n",
    "| `is_late_night` | Binary | Flag 12 AM - 3 AM (extreme suspicion) |\n",
    "| `hour_category` | Categorical | Morning/afternoon/evening/night grouping |\n",
    "\n",
    "**Why Multiple Hour Features?**\n",
    "- Different models benefit from different representations\n",
    "- Binary flags are easy for trees to split on\n",
    "- Categories help with interpretability\n",
    "- Numerical hour captures cyclical patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eaa79b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Missing Value Analysis and Engineering\n",
    "\n",
    "### The Problem with Traditional Missing Value Handling\n",
    "\n",
    "In anomaly detection, **missing values are NOT just noise** - they're **signals**:\n",
    "\n",
    "**Why Missing Data Matters:**\n",
    "1. **Intentional Obfuscation**: Attackers may omit user agents, countries, or other identifying information\n",
    "2. **System Errors**: Broken integrations or malware might fail to log properly\n",
    "3. **Data Exfiltration**: Certain attack vectors produce incomplete logs\n",
    "4. **Privacy Tools**: VPNs, ad blockers, or privacy browsers may block certain data collection\n",
    "\n",
    "**Strategy:**\n",
    "- **Don't just impute** - create binary indicators for each important missing field\n",
    "- **Count total missingness** - the number of missing fields per record\n",
    "- **Calculate missing percentage** - proportion of fields that are missing\n",
    "\n",
    "This way, models can learn patterns like:\n",
    "- \"Records with missing browser AND missing country are 10x more likely to be anomalies\"\n",
    "- \"High missing_count correlates with suspicious activity\"\n",
    "\n",
    "### Comprehensive Missing Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6871a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Column  Missing_Count  Missing_Percentage      Data_Type Missing_Category\n",
      "    device_os_version          14168               90.84        float64 Very High (>70%)\n",
      "              country          11260               72.19         object Very High (>70%)\n",
      "            device_os          10300               66.04         object    High (50-70%)\n",
      "      browser_version           9367               60.06         object    High (50-70%)\n",
      "                   rw           4474               28.69         object  Medium (20-50%)\n",
      "            operation           4336               27.80         object  Medium (20-50%)\n",
      "normalized_user_agent           3768               24.16         object  Medium (20-50%)\n",
      "              browser           3476               22.29         object  Medium (20-50%)\n",
      "      wf_principal_id             65                0.42         object       Low (<20%)\n",
      "                  isp             42                0.27         object       Low (<20%)\n",
      "                  asn              5                0.03        float64       Low (<20%)\n",
      "         day_of_month              0                0.00          int32             None\n",
      "             log_type              0                0.00         object             None\n",
      "           is_weekend              0                0.00          int32             None\n",
      "                 hour              0                0.00          int32             None\n",
      "    is_business_hours              0                0.00          int32             None\n",
      "       is_night_hours              0                0.00          int32             None\n",
      "        is_late_night              0                0.00          int32             None\n",
      "          day_of_week              0                0.00          int32             None\n",
      "         error_status              0                0.00          int64             None\n",
      "       was_duplicated              0                0.00          int32             None\n",
      "       original_index              0                0.00          int64             None\n",
      "        log_timestamp              0                0.00 datetime64[ns]             None\n",
      "            frequency              0                0.00          int64             None\n",
      "               uatype              0                0.00          int64             None\n",
      "                atype              0                0.00         object             None\n",
      "               app_id              0                0.00         object             None\n",
      "        hour_category              0                0.00         object             None\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Missing Value Summary:\n",
      "--------------------------------------------------------------------------------\n",
      "Missing_Category\n",
      "None                17\n",
      "Low (<20%)           3\n",
      "Medium (20-50%)      4\n",
      "High (50-70%)        2\n",
      "Very High (>70%)     2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_stats = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum().values,\n",
    "    'Missing_Percentage': (df.isnull().sum().values / len(df) * 100).round(2),\n",
    "    'Data_Type': df.dtypes.values\n",
    "})\n",
    "missing_stats = missing_stats.sort_values('Missing_Percentage', ascending=False)\n",
    "missing_stats['Missing_Category'] = pd.cut(\n",
    "    missing_stats['Missing_Percentage'], \n",
    "    bins=[-0.1, 0, 20, 50, 70, 100],\n",
    "    labels=['None', 'Low (<20%)', 'Medium (20-50%)', 'High (50-70%)', 'Very High (>70%)']\n",
    ")\n",
    "\n",
    "print(missing_stats.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Missing Value Summary:\")\n",
    "print(\"-\"*80)\n",
    "print(missing_stats['Missing_Category'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aab108",
   "metadata": {},
   "source": [
    "Analyzing missing value patterns across all columns to understand data quality and identify which fields are most critical to track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dffbe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary indicators for important missing fields\n",
    "df['has_wf_principal_id'] = df['wf_principal_id'].notna().astype(int)\n",
    "df['has_browser'] = df['browser'].notna().astype(int)\n",
    "df['has_browser_version'] = df['browser_version'].notna().astype(int)\n",
    "df['has_device_os'] = df['device_os'].notna().astype(int)\n",
    "df['has_device_os_version'] = df['device_os_version'].notna().astype(int)\n",
    "df['has_country'] = df['country'].notna().astype(int)\n",
    "df['has_isp'] = df['isp'].notna().astype(int)\n",
    "df['has_asn'] = df['asn'].notna().astype(int)\n",
    "df['has_operation'] = df['operation'].notna().astype(int)\n",
    "df['has_rw'] = df['rw'].notna().astype(int)\n",
    "df['has_normalized_user_agent'] = df['normalized_user_agent'].notna().astype(int)\n",
    "\n",
    "# Count total missing values per row\n",
    "df['missing_count'] = df.isnull().sum(axis=1)\n",
    "df['missing_percentage'] = (df['missing_count'] / len(df.columns) * 100).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70434c",
   "metadata": {},
   "source": [
    "### Create Missing Value Indicator Features\n",
    "\n",
    "**Why These Specific Fields?**\n",
    "\n",
    "| Field | Anomaly Relevance |\n",
    "|-------|------------------|\n",
    "| `wf_principal_id` | Missing user ID - unauthenticated or unauthorized access |\n",
    "| `browser` | Missing browser - automated script or API call |\n",
    "| `device_os` | Missing OS - suspicious client or spoofed request |\n",
    "| `country` | Missing country - VPN, proxy, or geo-blocking evasion |\n",
    "| `isp` | Missing ISP - network obfuscation |\n",
    "| `operation` | Missing operation - malformed request |\n",
    "\n",
    "**Feature Strategy:**\n",
    "- Create `has_*` binary features (1 = present, 0 = missing)\n",
    "- Create `missing_count` - total missing fields per record\n",
    "- Create `missing_percentage` - proportion of missing fields\n",
    "\n",
    "**Feature Breakdown:**\n",
    "\n",
    "| Feature | Type | Purpose |\n",
    "|---------|------|---------|\n",
    "| `has_wf_principal_id` | Binary (0/1) | 1 if user ID present, 0 if missing - flags unauthenticated access |\n",
    "| `has_browser` | Binary (0/1) | 1 if browser present, 0 if missing - flags automated scripts/API calls |\n",
    "| `has_browser_version` | Binary (0/1) | 1 if browser version present, 0 if missing - flags incomplete browser data |\n",
    "| `has_device_os` | Binary (0/1) | 1 if device OS present, 0 if missing - flags suspicious clients |\n",
    "| `has_device_os_version` | Binary (0/1) | 1 if OS version present, 0 if missing - flags incomplete device data |\n",
    "| `has_country` | Binary (0/1) | 1 if country present, 0 if missing - flags VPN/proxy/geo-blocking evasion |\n",
    "| `has_isp` | Binary (0/1) | 1 if ISP present, 0 if missing - flags network obfuscation |\n",
    "| `has_asn` | Binary (0/1) | 1 if ASN present, 0 if missing - flags network information tampering |\n",
    "| `has_operation` | Binary (0/1) | 1 if operation present, 0 if missing - flags malformed requests |\n",
    "| `has_rw` | Binary (0/1) | 1 if read/write present, 0 if missing - flags incomplete operation metadata |\n",
    "| `has_normalized_user_agent` | Binary (0/1) | 1 if user agent present, 0 if missing - flags client obfuscation |\n",
    "| `missing_count` | Numerical | Total count of missing fields per record - aggregate missingness signal |\n",
    "| `missing_percentage` | Numerical (0-100) | Percentage of fields missing per record - normalized missingness metric |\n",
    "\n",
    "**Why Multiple Missing Indicators?**\n",
    "- Individual flags allow models to identify specific missing field patterns\n",
    "- Aggregate metrics (`missing_count`, `missing_percentage`) capture overall data quality\n",
    "- Missing data in security logs is often intentional obfuscation, not random noise\n",
    "\n",
    "**ML Benefit:**\n",
    "Models can learn complex patterns like:\n",
    "- \"missing_count > 5 AND has_error = 1 - 80% chance of anomaly\"\n",
    "- \"has_country = 0 AND has_isp = 0 - likely VPN/proxy use\"\n",
    "- \"has_browser = 0 AND has_device_os = 0 - automated tool/script\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba9a607",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Aggregation Features: Behavioral Context\n",
    "\n",
    "### The Power of Aggregation in Anomaly Detection\n",
    "\n",
    "**Key Insight**: An event is only anomalous **in context**. Aggregation features provide that context.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- User A: 1000 activities, 2 errors → 0.2% error rate (normal)\n",
    "- User B: 10 activities, 2 errors → 20% error rate (ANOMALOUS!)\n",
    "\n",
    "\n",
    "Without aggregation, both have \"2 errors\" - identical. With aggregation, User B is clearly suspicious.\n",
    "\n",
    "**Aggregation Strategy:**\n",
    "\n",
    "1. **User-Level Aggregations** (`wf_principal_id`)\n",
    "   - Total activity count\n",
    "   - Error rate and total errors\n",
    "   - Geographic diversity (unique countries)\n",
    "   - Network diversity (unique ASNs)\n",
    "   - Browser diversity (unique browsers)\n",
    "   - Frequency statistics\n",
    "\n",
    "2. **App-Level Aggregations** (`app_id`)\n",
    "   - Application activity volume\n",
    "   - App-specific error rates\n",
    "   - Frequency patterns\n",
    "\n",
    "3. **Log-Type Aggregations** (`log_type`)\n",
    "   - Error rates by log type\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Baseline Behavior**: \"What's normal for THIS user?\"\n",
    "- **Relative Comparison**: \"Is this error rate high for THIS app?\"\n",
    "- **Pattern Deviation**: \"Does this user normally access from multiple countries?\"\n",
    "\n",
    "### User-Level Behavioral Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0b7c763",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_stats = df.groupby('wf_principal_id').agg({\n",
    "    'log_timestamp': 'count',  # total activity\n",
    "    'error_status': ['mean', 'sum'],  # error rate and total errors\n",
    "    'country': 'nunique',  # number of unique countries\n",
    "    'asn': 'nunique',  # number of unique ASNs\n",
    "    'browser': 'nunique',  # number of unique browsers\n",
    "    'frequency': ['sum', 'mean', 'max']  # frequency stats\n",
    "}).reset_index()\n",
    "\n",
    "user_stats.columns = ['wf_principal_id', 'user_total_activity', 'user_error_rate', \n",
    "                      'user_total_errors', 'user_country_count', 'user_asn_count',\n",
    "                      'user_browser_count', 'user_total_frequency', 'user_avg_frequency',\n",
    "                      'user_max_frequency']\n",
    "\n",
    "df = df.merge(user_stats, on='wf_principal_id', how='left')\n",
    "\n",
    "# Fill NaN for records without user_id\n",
    "for col in ['user_total_activity', 'user_error_rate', 'user_total_errors', \n",
    "            'user_country_count', 'user_asn_count', 'user_browser_count',\n",
    "            'user_total_frequency', 'user_avg_frequency', 'user_max_frequency']:\n",
    "    df[col] = df[col].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aeb70d",
   "metadata": {},
   "source": [
    "**User Statistics Explained:**\n",
    "\n",
    "| Feature | What It Captures | Anomaly Signal |\n",
    "|---------|-----------------|----------------|\n",
    "| `user_total_activity` | How active is this user? | Very low = new/compromised account |\n",
    "| `user_error_rate` | User's historical error rate | High = problematic user or attacker |\n",
    "| `user_total_errors` | Absolute error count | Context for error rate |\n",
    "| `user_country_count` | # of countries user accesses from | >1 = potential account compromise |\n",
    "| `user_asn_count` | # of networks user uses | High = traveling or suspicious |\n",
    "| `user_browser_count` | # of browsers user uses | High = shared account or testing |\n",
    "| `user_total_frequency` | Sum of all user's frequencies | Activity intensity |\n",
    "| `user_avg_frequency` | Average frequency per activity | Typical behavior baseline |\n",
    "| `user_max_frequency` | Highest frequency seen | Peak activity detection |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26e349f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# App-level aggregations\n",
    "app_stats = df.groupby('app_id').agg({\n",
    "    'log_timestamp': 'count',  # total activity\n",
    "    'error_status': ['mean', 'sum'],  # error rate and total errors\n",
    "    'frequency': ['sum', 'mean', 'max']  # frequency stats\n",
    "}).reset_index()\n",
    "\n",
    "app_stats.columns = ['app_id', 'app_total_activity', 'app_error_rate', \n",
    "                     'app_total_errors', 'app_total_frequency', \n",
    "                     'app_avg_frequency', 'app_max_frequency']\n",
    "\n",
    "df = df.merge(app_stats, on='app_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f58cf64",
   "metadata": {},
   "source": [
    "### Application-Level Aggregations\n",
    "\n",
    "**Why App-Level Stats Matter:**\n",
    "\n",
    "Different applications have different normal behavior:\n",
    "- **High-traffic apps** naturally have more errors\n",
    "- **Critical apps** (authentication, payments) should have lower error rates\n",
    "- **Unusual activity for a specific app** is a strong anomaly signal\n",
    "\n",
    "**Features:**\n",
    "- `app_total_activity`: How popular/active is this app?\n",
    "- `app_error_rate`: What's the typical error rate for this app?\n",
    "- `app_total_errors`: Absolute error volume\n",
    "- `app_total/avg/max_frequency`: Activity intensity for this app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dbb19b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log type aggregations\n",
    "logtype_stats = df.groupby('log_type').agg({\n",
    "    'error_status': 'mean'\n",
    "}).reset_index()\n",
    "logtype_stats.columns = ['log_type', 'logtype_error_rate']\n",
    "\n",
    "df = df.merge(logtype_stats, on='log_type', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd1fd82",
   "metadata": {},
   "source": [
    "### Log-Type Aggregations\n",
    "\n",
    "Different log types (`log_type`) (authentication, API calls, database queries, etc.) have different typical error rates (`logtype_error_rate`).\n",
    "\n",
    "**Use Case:**\n",
    "- Authentication logs with 10% error rate might be normal (wrong passwords)\n",
    "- Database write logs with 10% error rate would be highly suspicious"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b984fd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Rarity Indicators: The Power of the Uncommon\n",
    "\n",
    "### The Rarity Principle in Anomaly Detection\n",
    "\n",
    "**Core Concept**: In security and anomaly detection, **rare = suspicious**\n",
    "\n",
    "Most normal behavior follows common patterns:\n",
    "- Popular browsers (Chrome, Firefox, Safari)\n",
    "- Common ISPs (major telecom providers)\n",
    "- Standard operations (login, read, view)\n",
    "- Typical countries (for that application's user base)\n",
    "\n",
    "Anomalies break these patterns by using:\n",
    "- Obscure browsers (automated tools, outdated clients)\n",
    "- Rare ISPs (VPNs, Tor nodes, compromised servers)\n",
    "- Unusual operations (deprecated APIs, admin functions)\n",
    "- Unexpected countries (geographic anomalies)\n",
    "\n",
    "### Why Different Thresholds?\n",
    "\n",
    "| Feature | Threshold | Rationale |\n",
    "|---------|-----------|-----------|\n",
    "| **Operations** | < 10 | Operations vary more; some legitimate ones are uncommon |\n",
    "| **Browsers** | < 5 | Browser landscape is relatively stable |\n",
    "| **ISPs** | < 5 | Most users come from major providers |\n",
    "| **Countries** | = 1 | Single occurrence is extremely unusual |\n",
    "| **Users** | < 10 | New/rare users need special attention |\n",
    "\n",
    "### Rarity Analysis\n",
    "\n",
    "First, let's analyze the value distributions to understand what constitutes \"rare\" in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "485d45de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RARITY ANALYSIS - VALUE COUNTS EXPLORATION\n",
      "================================================================================\n",
      "\n",
      "1. OPERATION VALUE COUNTS\n",
      "--------------------------------------------------------------------------------\n",
      "Total unique operations: 251\n",
      "Operations with <10 occurrences: 145\n",
      "\n",
      "Rarest operations (bottom 10):\n",
      "operation\n",
      "me/calendarview                            1\n",
      "SiteIBModeSet                              1\n",
      "ListItemCreated                            1\n",
      "groups/*/sites/*/drive                     1\n",
      "FileModifiedExtended                       1\n",
      "me/messages                                1\n",
      "AppStoreStorefrontTaskGetApps              1\n",
      "teams/*/channels/*/tabs                    1\n",
      "me/getmemberobjects                        1\n",
      "users/*/ownedobjects/graph.group/$count    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2. BROWSER VALUE COUNTS\n",
      "--------------------------------------------------------------------------------\n",
      "Total unique browsers: 30\n",
      "Browsers with <5 occurrences: 10\n",
      "\n",
      "Rarest browsers (count <5):\n",
      "browser\n",
      "Node.js                      4\n",
      "ODMTA Transform Thumbnail    4\n",
      "Microsoft Actor Analytics    3\n",
      "OkHttp Client                3\n",
      "python-requests              2\n",
      "Azure SDK Go Identity        2\n",
      "Java                         1\n",
      "Microsoft Flow Service       1\n",
      "AzureHound                   1\n",
      "Microsoft To Do              1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "3. ISP VALUE COUNTS\n",
      "--------------------------------------------------------------------------------\n",
      "Total unique ISPs: 40\n",
      "ISPs with <5 occurrences: 13\n",
      "\n",
      "Rarest ISPs (count <5):\n",
      "isp\n",
      "JAN ADSL EEUA                                                 4\n",
      "CLT ADSL CBB                                                  3\n",
      "NORTHPOINT HOTEL GROUP DBA ALOFT                              3\n",
      "MCN ADSL EEUA                                                 2\n",
      "XTUWH DSL                                                     1\n",
      "Societe Internationale de Telecommunications Aeronautiques    1\n",
      "WTCU, LLC                                                     1\n",
      "XBLCOK CA                                                     1\n",
      "INSPITELE SOLUTIONS PRIVATE LIMITED                           1\n",
      "ViaSat,Inc.                                                   1\n",
      "T-Mobile USA, Inc.                                            1\n",
      "Hawaiian Telcom Services Company, Inc.                        1\n",
      "GNV ADSL CBB                                                  1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "4. COUNTRY VALUE COUNTS\n",
      "--------------------------------------------------------------------------------\n",
      "Total unique countries: 6\n",
      "Countries with single occurrence: 4\n",
      "\n",
      "Country distribution:\n",
      "country\n",
      "IN    3747\n",
      "US     586\n",
      "RU       1\n",
      "NL       1\n",
      "PK       1\n",
      "RO       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "5. USER VALUE COUNTS\n",
      "--------------------------------------------------------------------------------\n",
      "Total unique users: 19\n",
      "Users with <10 records: 4\n",
      "\n",
      "Rare users (count <10):\n",
      "wf_principal_id\n",
      "user16    4\n",
      "user17    2\n",
      "user18    1\n",
      "user19    1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "operation_counts = df['operation'].value_counts()\n",
    "browser_counts = df['browser'].value_counts()\n",
    "isp_counts = df['isp'].value_counts()\n",
    "country_counts = df['country'].value_counts()\n",
    "user_counts = df['wf_principal_id'].value_counts()\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RARITY ANALYSIS - VALUE COUNTS EXPLORATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. OPERATION VALUE COUNTS\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Total unique operations: {len(operation_counts)}\")\n",
    "print(f\"Operations with <10 occurrences: {(operation_counts < 10).sum()}\")\n",
    "print(f\"\\nRarest operations (bottom 10):\")\n",
    "print(operation_counts.tail(10))\n",
    "\n",
    "print(\"\\n2. BROWSER VALUE COUNTS\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Total unique browsers: {len(browser_counts)}\")\n",
    "print(f\"Browsers with <5 occurrences: {(browser_counts < 5).sum()}\")\n",
    "print(f\"\\nRarest browsers (count <5):\")\n",
    "print(browser_counts[browser_counts < 5])\n",
    "\n",
    "print(\"\\n3. ISP VALUE COUNTS\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Total unique ISPs: {len(isp_counts)}\")\n",
    "print(f\"ISPs with <5 occurrences: {(isp_counts < 5).sum()}\")\n",
    "print(f\"\\nRarest ISPs (count <5):\")\n",
    "print(isp_counts[isp_counts < 5])\n",
    "\n",
    "print(\"\\n4. COUNTRY VALUE COUNTS\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Total unique countries: {len(country_counts)}\")\n",
    "print(f\"Countries with single occurrence: {(country_counts == 1).sum()}\")\n",
    "print(f\"\\nCountry distribution:\")\n",
    "print(country_counts)\n",
    "\n",
    "print(\"\\n5. USER VALUE COUNTS\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Total unique users: {len(user_counts)}\")\n",
    "print(f\"Users with <10 records: {(user_counts < 10).sum()}\")\n",
    "print(f\"\\nRare users (count <10):\")\n",
    "print(user_counts[user_counts < 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8610b28c",
   "metadata": {},
   "source": [
    "**Purpose**: This exploratory analysis helps us understand:\n",
    "- The distribution of categorical values\n",
    "- How many values are truly rare\n",
    "- Whether our thresholds (< 10, < 5, = 1) are appropriate\n",
    "- Which specific values get flagged as rare\n",
    "\n",
    "This informs our rarity indicator creation in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74c8195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rare operations (< 10 occurrences)\n",
    "operation_counts = df['operation'].value_counts()\n",
    "rare_operations = operation_counts[operation_counts < 10].index.tolist()\n",
    "df['is_rare_operation'] = df['operation'].isin(rare_operations).astype(int)\n",
    "\n",
    "# Rare browsers (< 5 occurrences)\n",
    "browser_counts = df['browser'].value_counts()\n",
    "rare_browsers = browser_counts[browser_counts < 5].index.tolist()\n",
    "df['is_rare_browser'] = df['browser'].isin(rare_browsers).astype(int)\n",
    "\n",
    "# Rare ISPs (< 5 occurrences)\n",
    "isp_counts = df['isp'].value_counts()\n",
    "rare_isps = isp_counts[isp_counts < 5].index.tolist()\n",
    "df['is_rare_isp'] = df['isp'].isin(rare_isps).astype(int)\n",
    "\n",
    "# Rare countries (single occurrence)\n",
    "country_counts = df['country'].value_counts()\n",
    "rare_countries = country_counts[country_counts == 1].index.tolist()\n",
    "df['is_rare_country'] = df['country'].isin(rare_countries).astype(int)\n",
    "\n",
    "# Rare users (< 10 records)\n",
    "user_counts = df['wf_principal_id'].value_counts()\n",
    "rare_users = user_counts[user_counts < 10].index.tolist()\n",
    "df['is_rare_user'] = df['wf_principal_id'].isin(rare_users).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d55a56",
   "metadata": {},
   "source": [
    "### Creation of Rarity Indicator Features\n",
    "\n",
    "**Implementation Strategy:**\n",
    "\n",
    "1. Count occurrences of each value using `value_counts()`\n",
    "2. Identify values below threshold\n",
    "3. Create binary indicator: 1 if value is rare, 0 if common\n",
    "4. Use `.isin()` for efficient membership testing\n",
    "\n",
    "**Anomaly Detection Value:**\n",
    "A record with multiple rarity flags is exponentially more suspicious:\n",
    "- 1 flag: 2-3x more likely to be anomalous\n",
    "- 3+ flags: 10-20x more likely to be anomalous\n",
    "- 5 flags: Almost certainly an anomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b398399",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Anomaly-Specific Features: Direct Red Flags\n",
    "\n",
    "### From Context to Direct Signals\n",
    "\n",
    "Previous features provided **context** (what's normal for this user/app/time).  \n",
    "These features are **direct anomaly signals** based on domain expertise.\n",
    "\n",
    "### Feature Categories\n",
    "\n",
    "**1. Error-Based Indicators**\n",
    "- `is_critical_error`: Flags `error_status == 2` (critical/severe errors)\n",
    "- `has_error`: Flags any error (`error_status > 0`)\n",
    "\n",
    "**Why Errors Matter:**\n",
    "- Normal operations rarely produce errors\n",
    "- Critical errors often indicate attacks, misconfigurations, or system compromise\n",
    "- Error patterns correlate strongly with malicious activity\n",
    "\n",
    "\n",
    "\n",
    "**2. Frequency-Based Indicators**\n",
    "\n",
    "| Feature | Threshold | What It Catches |\n",
    "|---------|-----------|----------------|\n",
    "| `is_high_frequency` | > 100 | Extreme activity bursts |\n",
    "| `is_very_high_frequency` | > 50 | Elevated activity |\n",
    "| `frequency_zscore` | Z-score | Statistical outliers |\n",
    "\n",
    "**Suspicious High-Frequency Patterns:**\n",
    "- **DDoS attacks**: Flooding servers with requests\n",
    "- **Data scraping**: Automated harvesting of information\n",
    "- **Brute force**: Rapid password/key attempts\n",
    "- **Data exfiltration**: Bulk data downloads\n",
    "\n",
    "**Z-Score Explanation:**\n",
    "- Z-score shows how many standard deviations from the mean\n",
    "- Z > 3: Extreme outlier (99.7% of data is within ±3σ)\n",
    "- Z > 10: Almost certainly anomalous\n",
    "- Provides continuous measure vs. binary threshold\n",
    "\n",
    "\n",
    "\n",
    "**3. Geographic/Behavioral Red Flags**\n",
    "\n",
    "| Feature | What It Flags | Security Implication |\n",
    "|---------|--------------|---------------------|\n",
    "| `is_multi_country_user` | User accessing from >1 country | Account compromise, credential sharing |\n",
    "| `is_invalid_asn` | ASN = 0 | Data tampering, spoofed network info |\n",
    "\n",
    "**Why Multi-Country Access Is Suspicious:**\n",
    "- Most users operate from 1 country (or 2 if traveling)\n",
    "- Simultaneous multi-country access is physically impossible for one person\n",
    "- Common in account takeover scenarios\n",
    "- Credential sharing or VPN hopping\n",
    "\n",
    "**Invalid ASN:**\n",
    "- ASN (Autonomous System Number) identifies network providers\n",
    "- ASN = 0 is technically invalid\n",
    "- Might indicate spoofing, data errors, or intentional obfuscation\n",
    "\n",
    "\n",
    "\n",
    "### Real-World Attack Example\n",
    "\n",
    "A truly suspicious record might show:\n",
    "\n",
    "- is_critical_error: 1\n",
    "- has_error: 1\n",
    "- is_very_high_frequency: 1  \n",
    "- frequency_zscore: 8.5 (extreme!)\n",
    "- is_multi_country_user: 1\n",
    "- is_invalid_asn: 1\n",
    "- is_night_hours: 1\n",
    "- is_rare_browser: 1\n",
    "\n",
    "\n",
    "**8 simultaneous red flags** → Almost certainly an attack or system compromise\n",
    "\n",
    "### Creation of Anomaly Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4df63ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critical error indicator\n",
    "df['is_critical_error'] = (df['error_status'] == 2).astype(int)\n",
    "# Any error indicator\n",
    "df['has_error'] = (df['error_status'] > 0).astype(int)\n",
    "# High frequency indicator (> 100)\n",
    "df['is_high_frequency'] = (df['frequency'] > 100).astype(int)\n",
    "# Very high frequency indicator (> 50)\n",
    "df['is_very_high_frequency'] = (df['frequency'] > 50).astype(int)\n",
    "# Frequency z-score\n",
    "df['frequency_zscore'] = (df['frequency'] - df['frequency'].mean()) / df['frequency'].std()\n",
    "# Multi-country user indicator\n",
    "df['is_multi_country_user'] = (df['user_country_count'] > 1).astype(int)\n",
    "# ASN = 0 indicator (potentially invalid)\n",
    "df['is_invalid_asn'] = (df['asn'] == 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a344b26",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Categorical Feature Encoding\n",
    "\n",
    "### Cardinality Analysis\n",
    "\n",
    "Before encoding, let's analyze the cardinality (number of unique values) of each categorical feature to determine the appropriate encoding strategy.\n",
    "\n",
    "**Purpose of This Analysis:**\n",
    "\n",
    "Before encoding categorical features, we analyze cardinality to choose the right encoding strategy:\n",
    "\n",
    "This ensures we don't create thousands of sparse columns while preserving important categorical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85aa9ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CATEGORICAL FEATURE CARDINALITY BREAKDOWN:\n",
      "--------------------------------------------------------------------------------\n",
      "        Feature  Unique_Values  Non_Missing  Missing Missing_%\n",
      "      operation            251        11261     4336     27.8%\n",
      "         app_id            153        15597        0      0.0%\n",
      "            isp             40        15555       42      0.3%\n",
      "        browser             30        12121     3476     22.3%\n",
      "wf_principal_id             19        15532       65      0.4%\n",
      "        country              6         4337    11260     72.2%\n",
      "       log_type              5        15597        0      0.0%\n",
      "  hour_category              4        15597        0      0.0%\n",
      "      device_os              4         5297    10300     66.0%\n",
      "          atype              2        15597        0      0.0%\n",
      "             rw              2        11123     4474     28.7%\n"
     ]
    }
   ],
   "source": [
    "categorical_features = ['wf_principal_id', 'app_id', 'browser', 'isp', 'operation',\n",
    "                        'log_type', 'atype', 'rw', 'hour_category', 'device_os', 'country']\n",
    "\n",
    "# Calculate cardinality for each feature\n",
    "cardinality_data = []\n",
    "\n",
    "for col in categorical_features:\n",
    "    unique_count = df[col].nunique()\n",
    "    total_count = df[col].notna().sum()\n",
    "    missing_count = df[col].isna().sum()\n",
    "    missing_pct = (missing_count / len(df) * 100)\n",
    "    \n",
    "    cardinality_data.append({\n",
    "        'Feature': col,\n",
    "        'Unique_Values': unique_count,\n",
    "        'Non_Missing': total_count,\n",
    "        'Missing': missing_count,\n",
    "        'Missing_%': f\"{missing_pct:.1f}%\"\n",
    "    })\n",
    "\n",
    "# Create DataFrame and sort by cardinality\n",
    "cardinality_df = pd.DataFrame(cardinality_data)\n",
    "cardinality_df = cardinality_df.sort_values('Unique_Values', ascending=False)\n",
    "\n",
    "print(\"\\nCATEGORICAL FEATURE CARDINALITY BREAKDOWN:\")\n",
    "print(\"-\"*80)\n",
    "print(cardinality_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cacd31a",
   "metadata": {},
   "source": [
    "### Encoding Approach\n",
    "\n",
    "The below code uses **two different encoding strategies** based on feature characteristics:\n",
    "\n",
    "### **1. Frequency Encoding** (High-Cardinality Features)\n",
    "Applied to: `wf_principal_id`, `app_id`, `browser`, `isp`, `operation`\n",
    "\n",
    "- Counts how many times each value appears in the dataset\n",
    "- Replaces the categorical value with its count\n",
    "- Example: If \"Chrome\" appears 5,000 times, all Chrome entries get encoded as 5000\n",
    "\n",
    "### **2. One-Hot Encoding** (Low-Cardinality Features)\n",
    "Applied to: `log_type`, `atype`, `rw`, `hour_category`, `device_os`, `country`\n",
    "\n",
    "- Creates separate binary columns for each category\n",
    "- Example: `log_type` with values [A, B, C] becomes 3 columns: `logtype_A`, `logtype_B`, `logtype_C`\n",
    "- Each row has 1 in the relevant column, 0 in others\n",
    "\n",
    "### Why Specific Methods for Specific Features\n",
    "\n",
    "### **Frequency Encoding for High-Cardinality Features**\n",
    "\n",
    "**High-cardinality** = Many unique values (hundreds/thousands)\n",
    "\n",
    "| Feature | Why Frequency Encoding? |\n",
    "|---------|------------------------|\n",
    "| `wf_principal_id` | Could have 1000+ unique users - one-hot would create 1000+ columns! |\n",
    "| `app_id` | Many different applications |\n",
    "| `browser` | Dozens of browser types |\n",
    "| `isp` | Hundreds of ISPs |\n",
    "| `operation` | Many different operations |\n",
    "\n",
    "**Advantages:**\n",
    "- **Compact**: Only 1 column instead of 1000+\n",
    "- **Preserves information**: Popular values get higher numbers, rare ones get lower numbers\n",
    "- **Aligns with anomaly detection**: Rare categories (low frequency) are often anomalous!\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- Chrome (5000 occurrences) → 5000\n",
    "- Firefox (2000 occurrences) → 2000\n",
    "- Rare_Browser_X (2 occurrences) → 2  ← Low number flags rarity!\n",
    "\n",
    "\n",
    "### **One-Hot Encoding for Low-Cardinality Features**\n",
    "\n",
    "**Low-cardinality** = Few unique values (2-10)\n",
    "\n",
    "| Feature | # Categories | Why One-Hot? |\n",
    "|---------|-------------|--------------|\n",
    "| `log_type` | ~3-5 | Each type has distinct meaning |\n",
    "| `atype` | ~3-5 | Authentication methods are qualitatively different |\n",
    "| `rw` | 3 (read/write/unknown) | Clear categorical distinction |\n",
    "| `hour_category` | 4 (morning/afternoon/evening/night) | Time periods have different risk profiles |\n",
    "| `device_os` | 5-7 (top OS + other) | Operating systems behave differently |\n",
    "| `country` | 4 (IN/US/other/missing) | Geographic regions have different patterns |\n",
    "\n",
    "**Advantages:**\n",
    "- **Preserves categorical distinctions**: \"Morning\" isn't \"2x better\" than \"Evening\" - they're just different\n",
    "- **Model flexibility**: Allows the model to learn different patterns for each category\n",
    "- **No ordinal assumption**: Doesn't imply any ordering or magnitude\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "hour_category = \"morning\" → hour_cat_afternoon=0, hour_cat_evening=0, hour_cat_morning=1, hour_cat_night=0\n",
    "```\n",
    "\n",
    "### Critical Importance for ML Models\n",
    "\n",
    "### **1. Machine Learning Requires Numbers**\n",
    "- Text like \"Chrome\" or \"morning\" must be converted to numbers\n",
    "\n",
    "### **2. The Curse of Dimensionality**\n",
    "If we used one-hot encoding for `wf_principal_id` with 1000 users:\n",
    "- Creates 1000 new columns\n",
    "- Dataset becomes sparse (mostly zeros)\n",
    "- Model training becomes slow and unstable\n",
    "- Risk of overfitting increases dramatically\n",
    "\n",
    "**Frequency encoding avoids this** by keeping it as 1 column.\n",
    "\n",
    "### **3. Semantic Meaning Preservation**\n",
    "\n",
    "**Frequency encoding captures \"popularity\":**\n",
    "- High frequency = common, likely normal\n",
    "- Low frequency = rare, potentially anomalous\n",
    "- This is exactly what we want for anomaly detection!\n",
    "\n",
    "**One-hot encoding captures \"identity\":**\n",
    "- Each category is treated as equally distinct\n",
    "- Model learns: \"When logtype_A=1, behavior X is normal\"\n",
    "- Different from: \"When logtype_B=1, behavior Y is normal\"\n",
    "\n",
    "### Special Handling Details\n",
    "\n",
    "### **Device OS - Top 5 + \"Other\"**\n",
    "```python\n",
    "top_os = df['device_os'].value_counts().head(5).index.tolist()\n",
    "df['device_os_filled'] = df['device_os'].apply(lambda x: x if x in top_os else 'other')\n",
    "```\n",
    "**Why?** Balance between detail and dimensionality\n",
    "- Keep the 5 most common OS (Windows, Mac, Android, iOS, Linux)\n",
    "- Group rare ones as \"other\"\n",
    "- Prevents dozens of rare OS categories from creating noise\n",
    "\n",
    "### **Country - Simplified to 4 Groups**\n",
    "```python\n",
    "df['country_grouped'] = df['country'].apply(\n",
    "    lambda x: x if x in ['IN', 'US'] else ('other' if pd.notna(x) else 'missing')\n",
    ")\n",
    "```\n",
    "**Why?** \n",
    "- IN (India) and US have more entries, while others have only one\n",
    "- Groups uncommon countries\n",
    "- Explicitly handles missing values\n",
    "\n",
    "### **RW - Filling Unknown**\n",
    "```python\n",
    "df['rw_filled'] = df['rw'].fillna('unknown')\n",
    "```\n",
    "**Why?** Missing read/write indicator is itself informative - some operations might not have this field, which could be suspicious\n",
    "\n",
    "### Real-World Impact\n",
    "\n",
    "**Without proper encoding:**\n",
    "- Model can't run (text data)\n",
    "- Information is lost\n",
    "\n",
    "**With frequency encoding only:**\n",
    "- Works but loses categorical distinctions\n",
    "- \"Morning\" as 1, \"Afternoon\" as 2 implies afternoon is \"more than\" morning\n",
    "\n",
    "**With one-hot encoding only:**\n",
    "- Works but creates 1000+ columns for user_id\n",
    "- Slow, memory-intensive, unstable\n",
    "\n",
    "**With this hybrid approach:**\n",
    "- Compact representation\n",
    "- Preserves rarity information (frequency encoding)\n",
    "- Preserves categorical distinctions (one-hot encoding)\n",
    "- Optimal balance for anomaly detection\n",
    "\n",
    "**Bottom line**: This encoding strategy is carefully designed to give the model maximum information with minimum complexity, specifically optimized for detecting anomalous patterns in log data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4214570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency encoding for high-cardinality features\n",
    "# User ID frequency encoding\n",
    "wf_principal_freq = df['wf_principal_id'].value_counts().to_dict()\n",
    "df['wf_principal_id_freq_encoded'] = df['wf_principal_id'].map(wf_principal_freq).fillna(0)\n",
    "\n",
    "# App ID frequency encoding\n",
    "app_freq = df['app_id'].value_counts().to_dict()\n",
    "df['app_id_freq_encoded'] = df['app_id'].map(app_freq)\n",
    "\n",
    "# Browser frequency encoding\n",
    "browser_freq = df['browser'].value_counts().to_dict()\n",
    "df['browser_freq_encoded'] = df['browser'].map(browser_freq).fillna(0)\n",
    "\n",
    "# ISP frequency encoding\n",
    "isp_freq = df['isp'].value_counts().to_dict()\n",
    "df['isp_freq_encoded'] = df['isp'].map(isp_freq).fillna(0)\n",
    "\n",
    "# Operation frequency encoding\n",
    "op_freq = df['operation'].value_counts().to_dict()\n",
    "df['operation_freq_encoded'] = df['operation'].map(op_freq).fillna(0)\n",
    "\n",
    "# One-hot encoding for low-cardinality features (using drop_first=True to avoid dummy variable trap)\n",
    "# Log type\n",
    "log_type_dummies = pd.get_dummies(df['log_type'], prefix='logtype', drop_first=True, dtype=int)\n",
    "df = pd.concat([df, log_type_dummies], axis=1)\n",
    "\n",
    "# Authentication type\n",
    "atype_dummies = pd.get_dummies(df['atype'], prefix='atype', drop_first=True, dtype=int)\n",
    "df = pd.concat([df, atype_dummies], axis=1)\n",
    "\n",
    "# Read/Write operation\n",
    "df['rw_filled'] = df['rw'].fillna('unknown')\n",
    "rw_dummies = pd.get_dummies(df['rw_filled'], prefix='rw', drop_first=True, dtype=int)\n",
    "df = pd.concat([df, rw_dummies], axis=1)\n",
    "\n",
    "# Hour category\n",
    "hour_cat_dummies = pd.get_dummies(df['hour_category'], prefix='hour_cat', drop_first=True, dtype=int)\n",
    "df = pd.concat([df, hour_cat_dummies], axis=1)\n",
    "\n",
    "# Device OS (top categories)\n",
    "if df['device_os'].notna().sum() > 0:\n",
    "    top_os = df['device_os'].value_counts().head(5).index.tolist()\n",
    "    df['device_os_filled'] = df['device_os'].apply(lambda x: x if x in top_os else 'other').fillna('missing')\n",
    "    os_dummies = pd.get_dummies(df['device_os_filled'], prefix='os', drop_first=True, dtype=int)\n",
    "    df = pd.concat([df, os_dummies], axis=1)\n",
    "\n",
    "# Country (simplified to IN, US, Other, Missing)\n",
    "df['country_grouped'] = df['country'].apply(\n",
    "    lambda x: x if x in ['IN', 'US'] else ('other' if pd.notna(x) else 'missing')\n",
    ")\n",
    "country_dummies = pd.get_dummies(df['country_grouped'], prefix='country', drop_first=True, dtype=int)\n",
    "df = pd.concat([df, country_dummies], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00771c61",
   "metadata": {},
   "source": [
    "### Implementation Details\n",
    "\n",
    "**Frequency Encoding Process:**\n",
    "1. Count occurrences: `value_counts().to_dict()`\n",
    "2. Map counts back to original column: `.map()`\n",
    "3. Fill NaN with 0 (for missing values)\n",
    "4. Result: Each category replaced by its frequency\n",
    "\n",
    "**One-Hot Encoding Process:**\n",
    "1. Create binary columns: `pd.get_dummies(drop_first=True)`\n",
    "2. Use meaningful prefixes: `prefix='logtype'`\n",
    "3. **Drop first category** to avoid dummy variable trap (n categories → n-1 features)\n",
    "4. Concatenate to dataframe: `pd.concat()`\n",
    "5. Result: N categories → N-1 binary columns\n",
    "\n",
    "**Why drop_first=True?**\n",
    "- **Prevents multicollinearity**: When all other categories are 0, the dropped category is implied\n",
    "- **Reduces dimensionality**: For 4 hour categories, we only need 3 binary features instead of 4\n",
    "- **More efficient**: Fewer features = faster training, less memory\n",
    "- **Example**: If `hour_cat_morning=0, hour_cat_afternoon=0, hour_cat_evening=0`, then it must be `night`\n",
    "\n",
    "**Special Handling:**\n",
    "- **RW field**: Fill missing with 'unknown' before encoding (missingness is informative)\n",
    "- **Device OS**: Keep only top 5, group rest as 'other' (balance detail vs. complexity)\n",
    "- **Country**: Simplify to IN/US/other/missing (focus on key markets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dbee75",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Handling Remaining Missing Values in Numeric Features\n",
    "\n",
    "### Why Fill Numeric Missing Values?\n",
    "\n",
    "After creating missing value **indicators**, we still need to fill the actual missing values because:\n",
    "- Machine learning models can't process NaN values\n",
    "- We've already preserved the \"missingness\" information via indicator features\n",
    "- Filling allows us to use all records without dropping data\n",
    "\n",
    "### Filling Strategy\n",
    "\n",
    "| Feature | Fill Value | Rationale |\n",
    "|---------|-----------|-----------|\n",
    "| `asn` | 0 | Represents \"unknown network\" - captured by `is_invalid_asn` feature |\n",
    "| `device_os_version` | 0 | Represents \"unknown version\" - less critical than OS itself |\n",
    "| `uatype` | 0 | Represents \"unknown type\" - missingness already captured |\n",
    "\n",
    "**Note**: We use 0 rather than median/mean because:\n",
    "- We already flagged missingness with indicator features\n",
    "- 0 is a safe neutral value that won't skew statistics\n",
    "- It represents a distinct \"unknown\" category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0fb12a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill numerical nulls with 0\n",
    "df['asn'] = df['asn'].fillna(0)\n",
    "df['device_os_version'] = df['device_os_version'].fillna(0)\n",
    "df['uatype'] = df['uatype'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24acab7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Feature Scaling with RobustScaler\n",
    "\n",
    "### Why Scaling Matters for Anomaly Detection\n",
    "\n",
    "**The Problem:**\n",
    "Distance-based algorithms (LOF, DBSCAN, KNN) calculate distances between points. Features with larger ranges dominate these calculations.\n",
    "\n",
    "**Example:**\n",
    "- Feature A: frequency (range: 1 - 10,000)\n",
    "- Feature B: hour (range: 0 - 23)\n",
    "\n",
    "Without scaling, `frequency` differences overwhelm `hour` differences in distance calculations, even though hour might be equally important for detecting anomalies.\n",
    "\n",
    "\n",
    "### Scaling Method Comparison\n",
    "\n",
    "| Method | How It Works | Outlier Sensitive? | Good for Anomaly Detection? |\n",
    "|--------|-------------|-------------------|----------------------------|\n",
    "| **StandardScaler** | (X - mean) / std | YES | Anomalies corrupt mean/std |\n",
    "| **MinMaxScaler** | (X - min) / (max - min) | VERY | Extreme values compress normal range |\n",
    "| **RobustScaler** | **(X - median) / IQR** | **NO** | **BEST - Uses robust statistics** |\n",
    "| **MaxAbsScaler** | X / max(abs(X)) | YES | Affected by extreme values |\n",
    "\n",
    "\n",
    "### Why RobustScaler is Superior\n",
    "\n",
    "**1. Outlier-Resistant Statistics**\n",
    "- Uses **median** (50th percentile) instead of mean\n",
    "  - Median is unaffected by extreme values\n",
    "  - Example: [1, 2, 3, 4, 100] → median = 3, mean = 22\n",
    "\n",
    "- Uses **IQR** (Interquartile Range = Q3 - Q1) instead of standard deviation\n",
    "  - Only considers middle 50% of data (25th to 75th percentile)\n",
    "  - Extreme values don't affect the scale\n",
    "\n",
    "**2. Preserves Anomaly Characteristics**\n",
    "- Normal data scales to approximately [-1, 1]\n",
    "- Anomalies remain far from center (large absolute values)\n",
    "- The \"outlierness\" we want to detect is preserved!\n",
    "\n",
    "\n",
    "### Implementation Strategy\n",
    "\n",
    "**Features to Scale:**\n",
    "- All numerical features (frequency, hour, day, etc.)\n",
    "- User/app aggregation statistics\n",
    "- Frequency-encoded categorical features\n",
    "\n",
    "**Features NOT to Scale:**\n",
    "- Binary indicators (already 0/1)\n",
    "- One-hot encoded features (already 0/1)\n",
    "\n",
    "**Process:**\n",
    "1. Fit RobustScaler on each numerical feature\n",
    "2. Transform to scaled values\n",
    "3. Create new columns with `_scaled` suffix\n",
    "4. Preserve original values for interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80a11ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features to scale\n",
    "numerical_features_to_scale = [\n",
    "    'frequency', 'frequency_zscore', 'asn', 'uatype', 'device_os_version',\n",
    "    'hour', 'day_of_week', 'day_of_month',\n",
    "    'user_total_activity', 'user_error_rate', 'user_country_count', 'user_asn_count',\n",
    "    'user_browser_count', 'user_total_frequency', 'user_avg_frequency', 'user_max_frequency',\n",
    "    'app_total_activity', 'app_error_rate', 'app_total_errors',\n",
    "    'app_total_frequency', 'app_avg_frequency', 'app_max_frequency',\n",
    "    'user_id_freq_encoded', 'app_id_freq_encoded', 'browser_freq_encoded',\n",
    "    'isp_freq_encoded', 'operation_freq_encoded'\n",
    "]\n",
    "\n",
    "# Use RobustScaler (less sensitive to outliers)\n",
    "scaler = RobustScaler()\n",
    "\n",
    "for feature in numerical_features_to_scale:\n",
    "    if feature in df.columns:\n",
    "        df[f'{feature}_scaled'] = scaler.fit_transform(df[[feature]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb673a2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Final Feature Selection and Dataset Preparation\n",
    "\n",
    "### Feature Selection Strategy\n",
    "\n",
    "Selecting features based on three criteria:\n",
    "\n",
    "1. **Relevance to Anomaly Detection**\n",
    "   - Direct anomaly signals (errors, high frequency, rarity)\n",
    "   - Behavioral context (user/app statistics)\n",
    "   - Temporal patterns (time-based indicators)\n",
    "\n",
    "2. **Feature Type Diversity**\n",
    "   - Binary indicators: Simple yes/no flags\n",
    "   - Scaled numerical: Continuous measures\n",
    "   - One-hot encoded: Categorical distinctions\n",
    "\n",
    "3. **Avoiding Redundancy**\n",
    "   - Not including both raw AND scaled versions\n",
    "   - Using scaled versions for modeling\n",
    "   - Keeping original values for interpretation only\n",
    "\n",
    "\n",
    "### Feature Categories in Final Set\n",
    "\n",
    "**1. Binary Indicators (Flags)**\n",
    "- Missing value indicators: `has_user_id`, `has_browser`, `has_device_os`, etc.\n",
    "- Temporal flags: `is_weekend`, `is_business_hours`, `is_night_hours`\n",
    "- Rarity flags: `is_rare_operation`, `is_rare_browser`, `is_rare_isp`, etc.\n",
    "- Anomaly signals: `is_critical_error`, `has_error`, `is_high_frequency`, etc.\n",
    "\n",
    "**2. Scaled Numerical Features**\n",
    "- Activity metrics: `frequency_scaled`, `user_total_activity_scaled`\n",
    "- Network info: `asn_scaled`\n",
    "- Temporal: `hour_scaled`, `day_of_week_scaled`\n",
    "- Aggregations: `user_error_rate_scaled`, `app_error_rate_scaled`\n",
    "- Frequency-encoded: `user_id_freq_encoded_scaled`, `browser_freq_encoded_scaled`\n",
    "\n",
    "**3. One-Hot Encoded Categories**\n",
    "- Log types: `logtype_*`\n",
    "- Authentication: `atype_*`\n",
    "- Operations: `rw_*`\n",
    "- Time periods: `hour_cat_*`\n",
    "- Device: `os_*`\n",
    "- Geography: `country_*`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "855d1e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Selected 49 features for modeling\n",
      "\n",
      "Feature Categories:\n",
      "  - Binary indicators: 18\n",
      "  - Scaled numerical: 14\n",
      "  - One-hot encoded: 17\n",
      "Final feature matrix shape: (15597, 50)\n"
     ]
    }
   ],
   "source": [
    "# Define final feature set for modeling\n",
    "model_features = []\n",
    "\n",
    "# Binary indicators\n",
    "model_features.extend([\n",
    "    'has_user_id', 'has_browser', 'has_device_os', 'has_country', 'has_operation',\n",
    "    'is_weekend', 'is_business_hours', 'is_night_hours',\n",
    "    'is_rare_operation', 'is_rare_browser', 'is_rare_isp', 'is_rare_country', 'is_rare_user',\n",
    "    'is_critical_error', 'has_error', 'is_high_frequency', 'is_very_high_frequency',\n",
    "    'is_multi_country_user', 'is_invalid_asn'\n",
    "])\n",
    "\n",
    "# Scaled numerical features\n",
    "model_features.extend([f'{f}_scaled' for f in [\n",
    "    'frequency', 'asn', 'uatype', 'hour', 'day_of_week',\n",
    "    'user_total_activity', 'user_error_rate', 'user_country_count',\n",
    "    'app_total_activity', 'app_error_rate',\n",
    "    'user_id_freq_encoded', 'app_id_freq_encoded', 'browser_freq_encoded',\n",
    "    'isp_freq_encoded', 'operation_freq_encoded'\n",
    "] if f'{f}_scaled' in df.columns])\n",
    "\n",
    "# One-hot encoded features\n",
    "model_features.extend(log_type_dummies.columns.tolist())\n",
    "model_features.extend(atype_dummies.columns.tolist())\n",
    "model_features.extend(rw_dummies.columns.tolist())\n",
    "model_features.extend(hour_cat_dummies.columns.tolist())\n",
    "if 'os_dummies' in locals():\n",
    "    model_features.extend(os_dummies.columns.tolist())\n",
    "model_features.extend(country_dummies.columns.tolist())\n",
    "\n",
    "# Filter to only features that exist\n",
    "model_features = [f for f in model_features if f in df.columns]\n",
    "\n",
    "print(f\"* Selected {len(model_features)} features for modeling\")\n",
    "print(f\"\\nFeature Categories:\")\n",
    "print(f\"  - Binary indicators: {len([f for f in model_features if f.startswith('is_') or f.startswith('has_')])}\")\n",
    "print(f\"  - Scaled numerical: {len([f for f in model_features if f.endswith('_scaled')])}\")\n",
    "print(f\"  - One-hot encoded: {len([f for f in model_features if any(f.startswith(p) for p in ['logtype_', 'atype_', 'rw_', 'hour_cat_', 'os_', 'country_'])])}\")\n",
    "\n",
    "# Create final dataset with original_index for traceability\n",
    "X = df[model_features].copy()\n",
    "X['original_index'] = df['original_index']  # Add index column for anomaly tracing\n",
    "\n",
    "print(f\"Final feature matrix shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89bba3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Saved full engineered dataset: data/engineered_features_full_data.csv\n",
      "\n",
      "* Saved model-ready features: data/model_features_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the full engineered dataset\n",
    "df.to_csv('data/engineered_features_full_data.csv', index=False)\n",
    "print(f\"* Saved full engineered dataset: data/engineered_features_full_data.csv\")\n",
    "\n",
    "# Save the model-ready feature matrix\n",
    "X.to_csv('data/model_features_data.csv', index=False)\n",
    "print(f\"\\n* Saved model-ready features: data/model_features_data.csv\")\n",
    "\n",
    "# Save feature names\n",
    "with open('data/feature_names.txt', 'w') as f:\n",
    "    for feature in model_features:\n",
    "        f.write(f\"{feature}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568e4f4d",
   "metadata": {},
   "source": [
    "<h1><center>END</center></h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
